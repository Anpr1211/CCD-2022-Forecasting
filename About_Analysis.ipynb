{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import networkx as nx\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading dataset\n",
    "data = pd.read_csv(r\"<address>\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a corpus of keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [] # store key words/phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the selections\n",
    "selected = data[data.Result == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in selected.index:\n",
    "    kw = selected.about_keywords[i]\n",
    "    \n",
    "    # filter selections that have an About\n",
    "    if isinstance(kw, str):\n",
    "        words = ast.literal_eval(selected.about_keywords[i]) # convert to list\n",
    "        \n",
    "        for j in range(len(words)):\n",
    "            print(words[j][0])\n",
    "            keywords.append(words[j][0]) # add keywords to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the keywords to pass them through a K-Means\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(keywords)\n",
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elbow curve\n",
    "wcss = []\n",
    "for i in range(1,11):\n",
    "    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1,11),wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.savefig('elbow.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the clusters\n",
    "kmeans = KMeans(n_clusters = 5, n_init = 20, n_jobs = 1) # n_init(number of iterations for clsutering) n_jobs(number of cpu cores to use)\n",
    "kmeans.fit(X)\n",
    "# We look at 5 the clusters generated by k-means.\n",
    "common_words = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look at 5 the clusters generated by k-means.\n",
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-5:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store cluster labels in a variable\n",
    "clusters = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(clusters).value_counts()  # frequency of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating cosine similarity of the keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg') # load spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = []   # list of dicts to store the similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing common words because they lead to a high similarity score\n",
    "def remove_common_words(k1, k2):\n",
    "    \n",
    "    str1_words = set(k1.split())\n",
    "    str2_words = set(k2.split())\n",
    "    common = str1_words & str2_words\n",
    "    \n",
    "    for w in common:\n",
    "        \n",
    "        k1 = k1.replace(w, '')\n",
    "        k2 = k2.replace(w, '')\n",
    "    \n",
    "    return k1, k2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# finding the similarity scores\n",
    "for i in range(len(keywords)-1):\n",
    "    #print(i)\n",
    "    kw1, kw2 = remove_common_words(keywords[i].lower(), keywords[i+1].lower())\n",
    "    \n",
    "    if len(kw1)>1 and len(kw2)>1:\n",
    "        \n",
    "        t1 = nlp(kw1)\n",
    "        t2 = nlp(kw2)\n",
    "\n",
    "        tmp = {\"word1\":str(t1), \"word2\":str(t2), \"similarity\":t1.similarity(t2)}\n",
    "        print(i)\n",
    "        network.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list of dicts to dataframe\n",
    "network_data = pd.DataFrame(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the entities with similarity score < 0 since it is garbage value  (similarity score cannot be <0)\n",
    "network_data.drop(network_data[network_data.similarity < 0].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Network with Networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pandas dataframe as networkx graph\n",
    "G = nx.from_pandas_edgelist(network_data, \n",
    "                            source='word1', \n",
    "                            target='word2', \n",
    "                            edge_attr='similarity')\n",
    "print(\"No of unique characters:\", len(G.nodes))\n",
    "print(\"No of connections:\", len(G.edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of network with Pyviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(height='750px', width='100%', bgcolor='black', font_color='white')\n",
    "\n",
    "sources = network_data['word1']\n",
    "targets = network_data['word2']\n",
    "weights = network_data['similarity']\n",
    "\n",
    "edge_data = zip(sources, targets, weights)\n",
    "\n",
    "for e in edge_data:\n",
    "    src = e[0]\n",
    "    dst = e[1]\n",
    "    w = e[2]\n",
    "\n",
    "    net.add_node(src, src, title=src)\n",
    "    net.add_node(dst, dst, title=dst)\n",
    "    net.add_edge(src, dst, value=w, weight=w)\n",
    "\n",
    "neighbor_map = net.get_adj_list()\n",
    "\n",
    "# add neighbor data to node hover data\n",
    "for node in net.nodes:\n",
    "    node['value'] = len(neighbor_map[node['id']])\n",
    "    node['size'] = len(neighbor_map[node['id']])\n",
    "\n",
    "net.show('network_ccd.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
